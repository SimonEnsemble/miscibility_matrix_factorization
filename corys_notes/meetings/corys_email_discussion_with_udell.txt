Cory:

* I agree the XTX=I constraint in eqn. 5 for graph-Laplacian PCA isn't strictly needed, but wonder why vs. why not include it. looks like you omitted it from your objective in eqn. 6, too. 
* I guess I'm wrong about the spectral embedding of the vertices of the graph only considering 1-hop neighborhoods. though the objective function is deceptively written as a sum over edges, I think the vertex-vertex influences propagate throughout the entire graph.
* I think your paper opens the door to "kernel-"GLRM where you have a kernel function that describes similarity of pairs of examples?!
    * use kernel to get Gram matrix, normalize rows so they are transition probabilities, then treat that as the Laplacian matrix? then it would be like a "diffusion map" hooked up to matrix factorization?

Udell:

* Well, the rest of the objective depends only on XY, so insisting that X^T X = I just imposes a particular scaling on Y. I thought it would be better to avoid having to deal with any hard constraints, so omitted it. Of course, you can always renormalize at the end after finding X and Y. (You have to be a bit careful to see what implicit regularization this imposes, but it's hard to be certain what regularization is best, anyway... And there's something quite useful about the quadratic regularization on X and Y, since it is equivalent to trace norm regularization on XY which encourages XY to be low rank.)
* Yes, isn't it cool how it propagates?
* Yes, absolutely, kernel GLRM is allowed!
